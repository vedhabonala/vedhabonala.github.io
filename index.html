<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Vedhasree Bonala — Data Engineer</title>

  <!-- Font Awesome (optional icons) -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">

  <!-- Stylesheet -->
  <link rel="stylesheet" href="style.css" />
  <meta name="description" content="Vedhasree Bonala — Data Engineer portfolio. Cloud, ETL/ELT, lakehouse, streaming." />
</head>
<body>

  <!-- NAV -->
  <nav class="nav">
    <ul>
      <li><a href="#about">About</a></li>
      <li><a href="#experience">Experience</a></li>
      <li><a href="#projects">Projects</a></li>
      <li><a href="#skills">Skills & Certifications</a></li>
      <li><a href="#contact">Contact</a></li>
      <li><a href="data/resumee.pdf" target="_blank">CV</a></li>
    </ul>
  </nav>

  <!-- HERO -->
  <header id="about" class="hero">
    <div class="hero-inner">
      <div class="hero-text">
        <h1>Hello,</h1>
        <h2>I'm <span class="name">Vedhasree Bonala</span></h2>
        <p class="tagline">Data Engineer • Cloud & Big Data • AWS · Azure · GCP</p>

        <ul class="highlights">
          <li>Design and operate scalable ETL/ELT pipelines, lakehouse & warehouse architectures.</li>
          <li>Build robust streaming and batch ingestion with strong data quality and governance.</li>
          <li>Enable BI & analytics teams with curated, production-ready datasets and dashboards.</li>
        </ul>

        <p class="cta">
          <a class="btn" href="data/resume.pdf" target="_blank"><i class="fa fa-file"></i> Download CV</a>
          <a class="btn" href="https://github.com/vedhabonala" target="_blank"><i class="fab fa-github"></i> GitHub</a>
          <a class="btn" href="https://www.linkedin.com/in/vedhasreeb" target="_blank"><i class="fab fa-linkedin"></i> LinkedIn</a>
        </p>

        <p class="contact-mini">
          <strong>Phone</strong>: <a href="tel:+15019442776">+1 501-944-2776</a> &nbsp; • &nbsp;
          <strong>Email</strong>: <a href="mailto:vedasri996@gmail.com">vedasri996@gmail.com</a>
        </p>
      </div>

      <div class="hero-photo">
        <!-- Upload your headshot to data/profile.jpg -->
        <img src="data/profile.jpg" alt="Vedhasree Bonala — profile photo" />
      </div>
    </div>
  </header>

  <main class="container">
    <!-- ABOUT (full paragraphs you provided) -->
    <section class="section about-long">
      <h3>About me</h3>

      <p>
     I'm Vedhasree Bonala, a dedicated and detail-oriented Data Engineer passionate about building modern data ecosystems that help organizations transform raw information into meaningful, intelligent outcomes. I began my journey working with structured on-premises data systems, where I learned how to manage, optimize, and maintain traditional data platforms. As I advanced in my career, I embraced the power of cloud technologies, designing scalable, secure, and high-performance data pipelines that support real-time insights, analytics, and machine learning solutions.                    

      <p>
        Over the years, I’ve gained strong expertise in ETL/ELT development, big data processing, cloud data architectures, and data governance practices. I specialize in constructing resilient pipelines that ingest, clean, transform, and model datasets of all sizes—ensuring accuracy, reliability, and end-to-end data flow efficiency. My work spans across diverse data sources, structured and unstructured formats, and cloud environments such as Azure, AWS, Snowflake, and Databricks. I take pride in implementing best practices in data quality validation, metadata management, lineage tracking, and security controls, enabling teams to trust and utilize data confidently.
      </p>

      <p>
        Beyond building pipelines, I also enjoy enabling analytics and intelligence. I’ve worked on feature engineering, exploratory analysis, and predictive modeling, turning complex datasets into valuable business insights. By analyzing patterns, trends, and behaviors, I’ve helped teams make informed decisions, improve operational efficiency, and unlock new opportunities. Whether it’s designing a reliable data architecture, optimizing performance, or supporting ML workflows, I focus on delivering solutions that are automated, scalable, and aligned with real business goals.
      </p>

      <p>
        A blend of technical expertise, analytical thinking, and a passion for problem-solving drives everything I do. I collaborate closely with stakeholders, analysts, engineers, and architects to build data systems that are not only functional but transformational. My goal is simple: to create powerful data solutions that help organizations innovate, grow, and make smarter decisions. The best way to reach me is through email.
      </p>

      <p>
        The best way to reach me is through email.
      </p>

      
    </section>

    <!-- EXPERIENCE -->
    <section id="experience" class="section">
      <h3>Work Experience</h3>

      <article class="job">
        <h4>UnitedHealth Group — Data Engineer (Contract)</h4>
        <p class="meta">Jan 2025 – Present | Remote</p>
        <details>
          <summary>Key contributions</summary>
          <ul>
            <li>Built HIPAA-compliant ELT pipelines using AWS Glue, Lambda and PySpark for clinical, claims, and eligibility datasets.</li>
            <li>Designed S3-backed data lake layers with automated ingestion, schema evolution and partitioning for healthcare analytics.</li>
            <li>Implemented Redshift transformation patterns and query tuning to improve heavy analytical workloads.</li>
            <li>Established data quality checks, anomaly detection and reconciliation routines to improve trust in downstream reporting.</li>
            <li>Developed metadata-driven ingestion and lineage patterns for auditability and regulatory compliance.</li>
            <li>Applied encryption, masking and IAM controls when handling PHI/PII datasets and enabled BI teams with curated data marts.</li>
          </ul>
        </details>
      </article>

      <article class="job">
        <h4>Arkansas Summer Research Institute — Data Engineer Intern</h4>
        <p class="meta">May 2024 – Nov 2024 | Little Rock, AR</p>
        <details>
          <summary>Key contributions</summary>
          <ul>
            <li>Developed ADF and Databricks pipelines to ingest multi-format research data and prepare Delta Lake medallion layers.</li>
            <li>Built reproducible lakehouse patterns and PySpark jobs to support analytics and ML experiments.</li>
            <li>Created Synapse analytical models and Power BI / Tableau dashboards for research teams.</li>
          </ul>
        </details>
      </article>

      <article class="job">
        <h4>Wipro — Data Engineer</h4>
        <p class="meta">Aug 2022 – Dec 2023 | Hyderabad, India</p>
        <details>
          <summary>Key contributions</summary>
          <ul>
            <li>Engineered PySpark / Databricks and AWS Glue pipelines to ingest and transform enterprise datasets at scale.</li>
            <li>Designed dimensional models in Snowflake and Redshift and implemented transformation patterns for BI consumption.</li>
            <li>Automated multi-source ingestion with Apache Airflow and implemented production monitoring and alerting.</li>
            <li>Optimized Spark jobs using partitioning, broadcast joins and caching to improve runtime and reliability.</li>
            <li>Maintained data governance and integrated data quality checks into pipeline workflows.</li>
          </ul>
        </details>
      </article>

      <article class="job">
        <h4>Verizon — Data Engineer</h4>
        <p class="meta">Nov 2021 – Aug 2022 | Chennai, India</p>
        <details>
          <summary>Key contributions</summary>
          <ul>
            <li>Built BigQuery and Dataflow pipelines for telecom ingestion and analytics.</li>
            <li>Implemented Pub/Sub streaming ingestion patterns and reusable Dataflow templates for batch & streaming jobs.</li>
            <li>Optimized BigQuery designs with partitioning, clustering and materialized views to improve query performance.</li>
            <li>Automated schema validation and anomaly detection to ensure reliable analytics for Looker Studio dashboards.</li>
          </ul>
        </details>
      </article>
    </section>

    <!-- PROJECTS -->
    <section id="projects" class="section">
      <h3>Selected Projects</h3>

      <div class="project">
        <h4>Azure Lakehouse End-to-End Analytics (Azure)</h4>
        <p>Built ADF → ADLS → Databricks lakehouse with Delta layers and Synapse analytical models to support research and BI reporting.</p>
      </div>

      <div class="project">
        <h4>Real-Time Streaming Pipeline (GCP)</h4>
        <p>Implemented Pub/Sub → Dataflow → BigQuery streaming ingestion with schema validation, dead-letter handling and monitoring for reliable real-time analytics.</p>
      </div>

      <div class="project">
        <h4>Modern ELT Platform — Airflow + Snowflake + dbt (AWS / Snowflake)</h4>
        <p>Orchestrated Airflow DAGs, dbt models and Snowpipe ingestion to automate staging and modeling for consistent BI datasets.</p>
      </div>

      <div class="project">
        <h4>Real-Time Healthcare Events Pipeline (Kafka)</h4>
        <p>Built Kafka → Spark Structured Streaming → Snowflake pipeline for clinical and claims events with PHI protections, schema checks and secure ingestion.</p>
      </div>
    </section>

    <!-- SKILLS -->
    <section id="skills" class="section">
      <h3>Skills & Tools</h3>
      <ul class="skills-grid">
        <li>Python, PySpark, SQL, Bash</li>
        <li>AWS: S3, Glue, Lambda, Redshift, EMR, Athena</li>
        <li>Azure: ADF, Databricks, ADLS, Synapse</li>
        <li>GCP: Pub/Sub, Dataflow, BigQuery</li>
        <li>Snowflake, dbt, Airflow, Kafka, Delta Lake</li>
        <li>Hadoop, Hive, HDFS</li>
        <li>Power BI, Tableau, Looker Studio</li>
        <li>Git, Jenkins, Terraform, Docker, Kubernetes</li>
        <li>Data governance: IAM, masking, encryption, lineage, HIPAA handling</li>
      </ul>
    </section>

    <!-- CONTACT -->
    <section id="contact" class="section">
      <h3>Contact</h3>
      <p>Email: <a href="mailto:vedasri996@gmail.com">vedasri996@gmail.com</a></p>
      <p>Phone: <a href="tel:+15019442776">+1 501-944-2776</a></p>
      <p>
        <a href="https://www.linkedin.com/in/vedhasreeb" target="_blank"><i class="fab fa-linkedin"></i> LinkedIn</a>
        &nbsp;
        <a href="https://github.com/vedhabonala" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        &nbsp;
        <a href="https://vedhabonala.github.io/" target="_blank"><i class="fa fa-globe"></i> Portfolio</a>
      </p>
    </section>
  </main>

  <footer class="footer">
    <p>© 2025 Vedhasree Bonala</p>
  </footer>

</body>
</html>

