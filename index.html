<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Vedhasree Bonala — Data Engineer</title>

  <!-- Optional Google font (works in GitHub Pages / editors) -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700;800&display=swap" rel="stylesheet">

  <style>
    /* ---- Simple, explicit styling (no :root variables to avoid issues) ---- */
    * { box-sizing: border-box; }
    html,body{ height:100%; margin:0; padding:0; }
    body {
      font-family: "Inter", system-ui, -apple-system, "Segoe UI", Roboto, Arial, sans-serif;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
      color: #eaf9f9;
      background: linear-gradient(180deg,#071827,#04121a);
      line-height:1.6;
    }

    .wrap { max-width:1100px; margin:28px auto; padding:18px; }

    /* Header */
    header { display:flex; justify-content:space-between; align-items:center; gap:12px; margin-bottom:18px; }
    .brand { font-weight:800; color:#eaf9f9; }
    nav { display:flex; gap:10px; align-items:center; }
    nav a { color: rgba(255,255,255,0.85); text-decoration:none; padding:6px 8px; border-radius:6px; font-weight:600; font-size:14px; }
    nav a:hover { background: rgba(255,255,255,0.02); color:#12b7a8; }

    /* Hero section: centered big photo then intro text */
    .hero { background: linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.00)); border-radius:12px; padding:28px; box-shadow:0 10px 30px rgba(2,8,15,0.6); text-align:center; }
    .photo-wrap { width:360px; height:360px; margin:0 auto; border-radius:20px; overflow:hidden; border:1px solid rgba(255,255,255,0.04); background: rgba(255,255,255,0.02); box-shadow:0 12px 30px rgba(2,8,15,0.5); }
    .photo-wrap img { width:100%; height:100%; object-fit:cover; display:block; }

    .name-badge { display:inline-block; margin-top: -26px; background: rgba(0,0,0,0.42); padding:10px 16px; border-radius:999px; color:#eaf9f9; font-weight:700; border:1px solid rgba(255,255,255,0.03); }

    .greeting { margin-top:18px; max-width:920px; margin-left:auto; margin-right:auto; text-align:left; }
    @media (max-width:900px) { .greeting { text-align:center; } }

    .big-hi { font-size:56px; margin:0; line-height:0.95; font-weight:800; }
    .title { font-size:18px; margin:6px 0 16px; color:#9fd6d8; font-weight:700; }

    .lead { color: rgba(234,249,249,0.96); font-size:15.6px; margin:12px 0; text-align:justify; }

    .bullets { list-style:none; padding:0; margin:12px 0; display:flex; gap:12px; justify-content:flex-start; flex-wrap:wrap; }
    .bullets li { background: rgba(255,255,255,0.02); padding:8px 12px; border-radius:8px; font-weight:700; color:#eaf9f9; }

    .buttons { margin-top:14px; display:flex; gap:10px; justify-content:flex-start; flex-wrap:wrap; }
    .btn { padding:10px 14px; border-radius:8px; text-decoration:none; font-weight:800; font-size:14px; display:inline-block; }
    .btn-github { background:transparent; border:2px solid rgba(255,255,255,0.06); color:#eaf9f9; }
    .btn-linkedin { background:#12b7a8; color:#022; }

    .meta { margin-top:12px; color: rgba(255,255,255,0.8); font-size:14px; }

    /* Section containers */
    section.block { margin-top:22px; padding:16px; border-radius:10px; background: rgba(255,255,255,0.01); }
    h2.section-title { margin:0 0 8px; color:#9fd6d8; font-weight:800; font-size:16px; }

    /* details/summary styling */
    details { margin-top:10px; border-radius:8px; padding:10px; background: rgba(255,255,255,0.01); border:1px solid rgba(255,255,255,0.02); }
    details[open] { background: rgba(255,255,255,0.015); }
    details summary { cursor:pointer; list-style:none; font-weight:700; display:flex; justify-content:space-between; align-items:center; }
    details summary::-webkit-details-marker { display:none; } /* hide default triangle */
    .summary-meta { color:#9fd6d8; font-weight:600; margin-left:8px; font-size:13px; }
    details ul { margin:8px 0 0 18px; color: rgba(255,255,255,0.92); }
    details li { margin:6px 0; }

    /* Projects grid */
    .projects-grid { display:grid; grid-template-columns:1fr 1fr; gap:14px; margin-top:12px; }
    @media (max-width:980px) { .projects-grid { grid-template-columns:1fr; } }

    footer { text-align:center; margin-top:22px; color:rgba(255,255,255,0.7); font-size:13px; padding-bottom:26px; }

  </style>
</head>
<body>
  <div class="wrap">

    <!-- header -->
    <header>
      <div class="brand">WELCOME</div>
      <nav aria-label="Main navigation">
        <a href="#about">About</a>
        <a href="#experience">Experience</a>
        <a href="#projects">Projects</a>
        <a href="#developments">Developments</a>
        <a href="#skills">Skills & Certifications</a>
        <a href="data/resumee.pdf" target="_blank" rel="noopener">CV</a>
        <a href="#contact">Contact</a>
      </nav>
    </header>

    <!-- hero -->
    <section class="hero" id="about" aria-label="About">
      <div class="photo-wrap" role="img" aria-label="Profile photo of Vedhasree Bonala">
        <img src="data/profile.jpg" alt="Vedhasree Bonala — profile photo">
      </div>


      <div class="greeting">
        <!--header<div class="big-hi">Hello,</div>-->
        <h2>Hello,</h2> <div class="title">I'm <strong style="color:#12b7a8">Vedhasree Bonala</strong> — Data Engineer</div>

        <!-- exact paragraphs you asked to include (kept as provided) -->
        <p class="lead">
          A dedicated and detail-oriented Data Engineer passionate about building modern data ecosystems that help organizations transform raw information into meaningful, intelligent outcomes. I began my journey working with structured on-premises data systems, where I learned how to manage, optimize, and maintain traditional data platforms. As I advanced in my career, I embraced the power of cloud technologies, designing scalable, secure, and high-performance data pipelines that support real-time insights, analytics, and machine learning solutions.
        </p>

        <p class="lead">
          Over the years, I’ve gained strong expertise in ETL/ELT development, big data processing, cloud data architectures, and data governance practices. I specialize in constructing resilient pipelines that ingest, clean, transform, and model datasets of all sizes—ensuring accuracy, reliability, and end-to-end data flow efficiency. My work spans across diverse data sources, structured and unstructured formats, and cloud environments such as Azure, AWS, Snowflake, and Databricks. I take pride in implementing best practices in data quality validation, metadata management, lineage tracking, and security controls, enabling teams to trust and utilize data confidently.
        </p>

        <p class="lead">
          Beyond building pipelines, I also enjoy enabling analytics and intelligence. I’ve worked on feature engineering, exploratory analysis, and predictive modeling, turning complex datasets into valuable business insights. By analyzing patterns, trends, and behaviors, I’ve helped teams make informed decisions, improve operational efficiency, and unlock new opportunities. Whether it’s designing a reliable data architecture, optimizing performance, or supporting ML workflows, I focus on delivering solutions that are automated, scalable, and aligned with real business goals.
        </p>

        <p class="lead">
          A blend of technical expertise, analytical thinking, and a passion for problem-solving drives everything I do. I collaborate closely with stakeholders, analysts, engineers, and architects to build data systems that are not only functional but transformational. My goal is simple: to create powerful data solutions that help organizations innovate, grow, and make smarter decisions.
        </p>

        <p class="lead">The best way to reach me is through email.</p>

        <div class="buttons" aria-hidden="true">
          <a class="btn btn-github" href="https://github.com/vedhabonala" target="_blank" rel="noopener">GitHub</a>
          <a class="btn btn-linkedin" href="https://www.linkedin.com/in/vedhasreeb" target="_blank" rel="noopener">LinkedIn</a>
        </div>

        <div class="meta">
          Phone: <a href="tel:+15019442776">+1 501-944-2776</a> &nbsp; • &nbsp;
          Email: <a href="mailto:vedasri996@gmail.com">vedasri996@gmail.com</a>
        </div>
      </div>
    </section>

    <!-- Experience (details show/hide) -->
    <section class="block" id="experience" aria-labelledby="experience-heading">
      <h2 class="section-title" id="experience-heading">WORK EXPERIENCE</h2>

      <details>
        <summary>
          UnitedHealth Group — Data Engineer <span class="summary-meta">Jan 2025 - Present</span>
        </summary>
        <ul>
          <li>Engineered HIPAA-compliant AWS ELT pipelines using Glue, Lambda, and PySpark to process high-volume clinical, claims, and eligibility datasets.</li>
          <li>Designed and maintained S3-based data lake layers with automated ingestion, schema evolution, and optimized partitioning strategies tailored for healthcare analytics.</li>
          <li>Implemented Redshift transformation logic using distribution/sort keys, query pruning, and workload management (WLM).</li>
          <li>Implemented data quality frameworks including validation rules, anomaly detection, referential integrity checks, and reconciliation routines.</li>
          <li>Developed metadata-driven ingestion patterns supporting auditability, lineage tracking, and compliance.</li>
          <li>Optimized PySpark workloads for transforming PHI/PII-sensitive datasets with encryption, masking, and secure IAM-based access controls.</li>
          <li>Built curated, analytics-ready data marts enabling actuarial, provider insights, utilization management, and operational reporting teams.</li>
          <li>Collaborated with clinical analytics, BI, and product teams to translate healthcare requirements into scalable data models and pipelines.</li>
        </ul>
      </details>

      <details>
        <summary>
          Arkansas Summer Research Institute — Data Engineer Intern <span class="summary-meta">May 2024 - Nov 2024</span>
        </summary>
        <ul>
          <li>Developed Azure-based ETL/ELT pipelines using ADF, Databricks, and PySpark, processing multi-format datasets.</li>
          <li>Implemented Lakehouse architecture leveraging ADLS, Delta Lake, and curated medallion layers to support ML and analytics teams.</li>
          <li>Configured automated ADF ingestion pipelines integrating APIs, Blob Storage, and SQL sources with monitoring and alerting.</li>
          <li>Modeled analytical layers in Synapse using star-schema and incremental design patterns for research-based insights.</li>
          <li>Created interactive dashboards in Power BI and Tableau to help research teams analyze experimental and statistical results.</li>
        </ul>
      </details>

      <details>
        <summary>
          Wipro — Data Engineer <span class="summary-meta">Aug 2022 - Dec 2023</span>
        </summary>
        <ul>
          <li>Engineered large-scale ETL pipelines using PySpark, Databricks, and AWS Glue to ingest, transform, and curate multi-terabyte enterprise datasets.</li>
          <li>Designed and optimized Snowflake/Redshift dimensional models (star and snowflake schemas).</li>
          <li>Developed scalable ingestion frameworks orchestrated through Apache Airflow, automating cross-source loads.</li>
          <li>Improved PySpark pipeline performance with partitioning, broadcast joins, caching, and compression techniques.</li>
          <li>Processed large-scale datasets using Hadoop, Hive, and HDFS and executed Spark-based batch workloads on AWS EMR.</li>
          <li>Constructed and fine-tuned complex SQL transformations and stored procedures across PostgreSQL, Snowflake, and Redshift.</li>
          <li>Integrated data quality checks including null profiling, anomaly detection, and referential integrity validation into Airflow-managed workflows.</li>
        </ul>
      </details>

      <details>
        <summary>
          Verizon — Data Engineer <span class="summary-meta">Nov 2021 - Aug 2022</span>
        </summary>
        <ul>
          <li>Developed GCP-native ETL pipelines using BigQuery, Dataflow, and Cloud Storage to ingest, transform, and curate large-scale telecom datasets.</li>
          <li>Implemented Pub/Sub–based streaming ingestion for near-real-time processing of event logs and network activity.</li>
          <li>Optimized BigQuery SQL transformations using partitioning, clustering, materialized views, and window functions.</li>
          <li>Built reusable Dataflow templates for batch and streaming pipelines and automated data quality checks.</li>
          <li>Supported on-premises to GCP cloud migration by redesigning ingestion routes and improving scalability.</li>
          <li>Partnered with analytics and BI teams to deliver curated datasets powering Looker Studio dashboards.</li>
        </ul>
      </details>
    </section>

    <!-- Projects (details drop-downs) -->
    <section class="block" id="projects" aria-labelledby="projects-heading">
      <h2 class="section-title" id="projects-heading">PROJECTS</h2>

      <div class="projects-grid">
        <!-- Card 1 -->
        <div>
          <strong>Azure Lakehouse End-to-End Analytics (Azure)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Feb 2024</div>
          <details>
            <summary>Details</summary>
            <ul>
              <li>Built a Lakehouse using ADF, ADLS, Databricks, and Delta Lake to support scalable ingestion, schema evolution, and PySpark transformations for multi-format datasets.</li>
              <li>Modeled analytical views in Synapse with star schema and incremental loads, enabling fast Tableau reporting for research and analytics teams.</li>
            </ul>
          </details>
        </div>

        <!-- Card 2 -->
        <div>
          <strong>Real-Time Streaming Pipeline (GCP: Pub/Sub → Dataflow → BigQuery)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">May 2024</div>
          <details>
            <summary>Details</summary>
            <ul>
              <li>Created a real-time Pub/Sub → Dataflow → BigQuery pipeline with windowing and partitioning for analytics-ready storage.</li>
              <li>Added schema validation, dead-letter handling, and Cloud Functions monitoring to ensure reliable streaming ingestion.</li>
            </ul>
          </details>
        </div>

        <!-- Card 3 -->
        <div>
          <strong>Modern ELT Platform using Airflow, Snowflake and dbt</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Sep 2023</div>
          <details>
            <summary>Details</summary>
            <ul>
              <li>Built an Airflow-driven ELT system using Snowflake and dbt to automate ingestion, staging, and transformation workflows.</li>
              <li>Implemented Snowpipe ingestion and dbt models for staging and dimensional layers, supporting consistent and high-quality BI datasets.</li>
            </ul>
          </details>
        </div>

        <!-- Card 4 -->
        <div>
          <strong>Real-Time Healthcare Events Pipeline (Kafka → Spark → Snowflake)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Dec 2024</div>
          <details>
            <summary>Details</summary>
            <ul>
              <li>Built a real-time healthcare pipeline using Kafka for claims and clinical events and processed streams with Spark Structured Streaming.</li>
              <li>Implemented Snowpipe ingestion with schema checks, PHI/PII protections, and HIPAA-aligned access controls.</li>
            </ul>
          </details>
        </div>

        <!-- Additional 6 projects (kept compact) -->
        <div>
          <strong>Serverless YouTube Analytics Pipeline (AWS)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Oct 2021</div>
          <details>
            <summary>Details</summary>
            <ul><li>Serverless ingestion using API pulls, Lambda transforms, S3 staging and Athena/Redshift for analytics.</li></ul>
          </details>
        </div>

        <div>
          <strong>IoT Sensor Analytics (Azure Event Hubs → Databricks)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Nov 2022</div>
          <details>
            <summary>Details</summary>
            <ul><li>Ingested telemetry into Delta Lake; streaming transforms and time-window aggregates for monitoring.</li></ul>
          </details>
        </div>

        <div>
          <strong>End-to-End Lakehouse (dbt + Snowflake + S3)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Feb 2023</div>
          <details>
            <summary>Details</summary>
            <ul><li>S3 landing, Snowflake ingestion, dbt transformations and materialized marts for analytics teams.</li></ul>
          </details>
        </div>

        <div>
          <strong>Predictive Sales Forecasting (BigQuery ML)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Jun 2023</div>
          <details>
            <summary>Details</summary>
            <ul><li>Feature-engineered BigQuery datasets and scheduled forecasting models in BigQuery ML.</li></ul>
          </details>
        </div>

        <div>
          <strong>Real-time Log Analytics (Kafka → Elastic → Kibana)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Feb 2022</div>
          <details>
            <summary>Details</summary>
            <ul><li>Streamed logs via Kafka into Elastic and built Kibana dashboards with alerting and retention policies.</li></ul>
          </details>
        </div>

        <div>
          <strong>Finance Data Warehouse (Azure Synapse)</strong>
          <div style="color:#cfeff0; font-weight:600; margin-top:6px;">Jul 2022</div>
          <details>
            <summary>Details</summary>
            <ul><li>Designed Synapse models and ETL pipelines with partitioning, incremental loads and role-based access.</li></ul>
          </details>
        </div>
      </div>
    </section>

    <!-- Developments -->
    <section class="block" id="developments" aria-labelledby="dev-heading">
      <h2 class="section-title" id="dev-heading">Developments</h2>
      <ul style="margin-left:18px">
        <li>Started Working as a Data Engineering at <a href="https://www.unitedhealthgroup.com" target="_blank" rel="noopener">UnitedHealth Group</a></li>
        <li>Worked as Data Engineer at <a href="https://www.arkansasedc.com/arkansas-nsf-epscor/division/data-analytics-that-are-robust-trusted-(dart)/arkansas-summer-research-institute" target="_blank" rel="noopener"> Arkansas Summer Research Institute</a></li>
        <li>Worked as Data Engineer at <a href="https://www.wipro.com/" target="_blank" rel="noopener">Wipro</a></li>
        <li>Worked as Data Engineer at <a href="https://www.verizon.com/business/solutions/it-infrastructure/" target="_blank" rel="noopener">Verizon</a></li>
      </ul>
    </section>

    <!-- Skills & Certifications -->
    <section class="block" id="skills" aria-labelledby="skills-heading">
      <h2 class="section-title" id="skills-heading">Skills & Certifications</h2>
      <div style="display:flex;gap:20px;flex-wrap:wrap;margin-top:10px">
        <div style="min-width:260px;flex:1">
          <strong>Programming & Scripting:</strong><br>Python, Scala, SQL, C, C++, Java, PySpark, Bash, Shell Scripting<br><br>
          <strong>AWS:</strong><br>S3, Glue, Glue Crawler, Lambda, Step Functions, CloudWatch, SQS, SNS, IAM, EMR, Athena, Event Bridge, RDS<br><br>
          <strong>Azure:</strong><br>ADF, ADLS Gen2, Synapse Analytics, Blob storage
        </div>
        <div style="min-width:260px;flex:1">
          <strong>GCP:</strong><br>BigQuery, Dataflow, Pub/Sub, Dataproc, Cloud Storage, Looker Studio<br><br>
          <strong>Data Engineering & Big Data:</strong><br>Spark, Kafka, Airflow, Delta Lake, Lakehouse, Hadoop, Hive, HDFS, dbt<br><br>
          <strong>Certifications:</strong><br>AWS Certified Solutions Architect – Associate<br>Microsoft Certified: Azure AI Fundamentals
        </div>
      </div>
    </section>

    <!-- Contact -->
    <section class="block" id="contact" aria-labelledby="contact-heading">
      <h2 class="section-title" id="contact-heading">Contact</h2>
      <p style="margin-top:8px">
        <strong>Phone:</strong> <a href="tel:+15019442776">+1 501-944-2776</a><br>
        <strong>Email:</strong> <a href="mailto:vedasri996@gmail.com">vedasri996@gmail.com</a><br>
        <strong>LinkedIn:</strong> <a href="https://www.linkedin.com/in/vedhasreeb" target="_blank" rel="noopener">linkedin.com/in/vedhasreeb</a><br>
        <strong>GitHub:</strong> <a href="https://github.com/vedhabonala" target="_blank" rel="noopener">github.com/vedhabonala</a>
      </p>
    </section>

  </div>

  <script>
    // dynamic year
    document.getElementById('yr').textContent = new Date().getFullYear();

    // accessible details toggles (aria-expanded reflects open state)
    document.querySelectorAll('details').forEach(function(d){
      var s = d.querySelector('summary');
      if(!s) return;
      s.setAttribute('role','button');
      s.setAttribute('aria-expanded', d.hasAttribute('open') ? 'true' : 'false');
      s.addEventListener('click', function(){
        setTimeout(function(){ s.setAttribute('aria-expanded', d.hasAttribute('open') ? 'true' : 'false'); }, 10);
      });
    });
  </script>
</body>
</html>
